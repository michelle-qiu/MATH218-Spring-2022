{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Make a copy of this notebook (File menu -> Make a Copy...)\n",
    "\n",
    "##### Throughout this week's homework, use your LUdet(A) function that you wrote in lab.\n",
    "\n",
    "### Homework Question 1\n",
    "\n",
    "1. Implement a function called *Cramers(A,v)* that solves the matrix equation $Ax=v$ using Cramer's Rule. If you need a reminder of Cramer's Rule, [see here](https://en.wikipedia.org/wiki/Cramer%27s_rule).<br><br>\n",
    "\n",
    "1. Consider the matrix equation $Ax=v$ where <br><br>\n",
    "   \n",
    "   $$A=\\begin{bmatrix}-4 & 1 & -9 &  4 &  4 &  1 &  9 & -6\\\\\n",
    "-1 & -3 & -8 & -2 & -4 &  4 &  2 &  7\\\\\n",
    " 3 &  6 & -8 &  1 &  0 &  1 &  2 &  5\\\\\n",
    "-10 & -4 & -1 & -4 &  2 & -8 &  3 &  2\\\\\n",
    "-6 &  7 & -4 & -6 &  2 &  1 & -4 &  8\\\\\n",
    " 6 &  6 &  5 &  8 & -7 &  9 & -1 &  9\\\\\n",
    " 1 & -10 &  2 & -6 & -8 & -9 & -4 &  0\\\\\n",
    " 6 &  1 & -4 &  3 &  8 &  3 &  3 &  5\\end{bmatrix}\\mbox{ and }v=\\begin{bmatrix} 28 \\\\ 40 \\\\ 45 \\\\ -18 \\\\ 26 \\\\ 114 \\\\ -125 \\\\ 102\\end{bmatrix}$$<br><br> \n",
    " \n",
    "      Solve these using both your Cramer's Rule function and previous solution techniques. Run timing comparisons and comment on your results. For your convenience, here is code for those arrays:\n",
    "      \n",
    "```python\n",
    "A = np.array([[4 , 1 , -9 ,  4 ,  4 ,  1 ,  9 , -6],[1 , -3 , -8 , -2 , -4 ,  4 ,  2 ,  7],[3 ,  6 , -8 ,  1 ,  0 ,  1 ,  2 ,  5],[10 , -4 , -1 , -4 ,  2 , -8 ,  3 ,  2],[6 ,  7 , -4 , -6 ,  2 ,  1 , -4 ,  8],[6 ,  6 ,  5 ,  8 , -7 ,  9 , -1 ,  9],[1 , -10 ,  2 , -6 , -8 , -9 , -4 ,  0],[6 ,  1 , -4 ,  3 ,  8 ,  3 ,  3 ,  5]])\n",
    "\n",
    "v=np.array([28,40,45,-18,26,114,-125,102])\n",
    "```\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.  1.  2.  3.  4.  5.  6.  7.]\n",
      "[  28.   40.   45.  -18.   26.  114. -125.  102.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rref import rref\n",
    "\n",
    "from Qiureferencefunctions import LU,swaprows,rowaddmult, fwdsub, backsub\n",
    "def Cramers(A,v):\n",
    "    d = det(A)\n",
    "    rows = A.shape[0]\n",
    "    ret = np.zeros((rows))\n",
    "    for i in range (0,rows):\n",
    "        C = np.zeros((rows,rows))\n",
    "        C[:,:i] = A[:,:i]\n",
    "        C[:,i] = v\n",
    "        C[:,i+1:] = A[:,i+1:]\n",
    "        ret[i] = det(C)/d\n",
    "    return ret\n",
    "A = np.array([[-4 , 1 , -9 ,  4 ,  4 ,  1 ,  9 , -6],[-1 , -3 , -8 , -2 , -4 ,  4 ,  2 ,  7],[3 ,  6 , -8 ,  1 ,  0 ,  1 ,  2 ,  5],[-10 , -4 , -1 , -4 ,  2 , -8 ,  3 ,  2],[-6 ,  7 , -4 , -6 ,  2 ,  1 , -4 ,  8],[6 ,  6 ,  5 ,  8 , -7 ,  9 , -1 ,  9],[1 , -10 ,  2 , -6 , -8 , -9 , -4 ,  0],[6 ,  1 , -4 ,  3 ,  8 ,  3 ,  3 ,  5]])\n",
    "v=np.array([28,40,45,-18,26,114,-125,102])\n",
    "sol = Cramers(A,v)\n",
    "print(sol)\n",
    "print(A@sol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.07 s ± 202 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "441 µs ± 52.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit Cramers(A,v)\n",
    "%timeit LUSolve(A,v)\n",
    "#The Cramers method is a lot slower compared to using LU decomposition to solve for v. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LUdet(A):\n",
    "    rows,cols = A.shape\n",
    "    copy = A.copy()\n",
    "    pivotcol = 0\n",
    "    pivotrow = 0\n",
    "    i = 1\n",
    "    numswaps =0\n",
    "    zero = np.zeros((rows,cols))\n",
    "    perm = np.eye(rows)\n",
    "    cool = (copy,zero,perm) #U,L,P, \n",
    "    while((pivotcol<cols) & (pivotrow<rows)):\n",
    "        while(i<rows):\n",
    "            maxe = np.argmax(abs(copy[pivotrow:,pivotcol])) +pivotrow\n",
    "            if (maxe > pivotrow):\n",
    "                swaprows(perm,maxe,pivotrow)\n",
    "                swaprows(zero,maxe,pivotrow)\n",
    "                numswaps+=1\n",
    "                copyrow = (copy[pivotrow]).copy();\n",
    "                copy[pivotrow] = (copy[maxe]).copy();\n",
    "                copy[maxe] = copyrow;\n",
    "            multval = (-1*copy[i,pivotcol])/(copy[pivotrow,pivotcol])\n",
    "            rowaddmult(copy,pivotrow,i,(multval))\n",
    "            zero[i,pivotrow] = -multval\n",
    "            i+=1\n",
    "        pivotcol+=1\n",
    "        pivotrow+=1\n",
    "        i = pivotrow+1\n",
    "    np.fill_diagonal(zero,1.)\n",
    "    Udet = np.prod(np.diag(copy))\n",
    "    numswaps = numswaps%2\n",
    "    Pdet = (-1)**numswaps\n",
    "    return Udet*Pdet\n",
    "def LUSolve(A,v):\n",
    "    U,L,P = LU(A)\n",
    "    v = P@v\n",
    "    y = fwdsub(L,v)\n",
    "    x = backsub(U,y)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework Question 2\n",
    "\n",
    "The inverse $A^{-1}$ of a matrix can be found using the *cofactor matrix*. If $A$ is a matrix<br><br> $$A=\\begin{bmatrix}a_{1,1} & \\cdots & a_{1,n} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{n,1} & \\cdots & a_{n,n}\\end{bmatrix}$$<br><br> then its *cofactor matrix* is defined to be $$adj(A)=\\begin{bmatrix}C_{1,1} & \\cdots & C_{1,n} \\\\ \\vdots & \\ddots & \\vdots \\\\ C_{n,1} & \\cdots & C_{n,n}\\end{bmatrix}^T,$$<br><br> where $C_{i,j}$ is the $(i,j)$ cofactor of $a_{i,j}$ (note the transpose sign!).<br><br>\n",
    "\n",
    "1. Find by hand the cofactor matrix of <br><br>$$A=\\begin{bmatrix} -2 & 11 & 48 \\\\-2 & 11 & 49 \\\\3 & -16 & -70\n",
    "\\end{bmatrix}.$$<br><br>\n",
    "1. Find the determinant of $A$, and show that $A\\cdot adj(A)=det(A)\\cdot I_3$ in this case (where $I_3$ is the $3\\times 3$ identity matrix).<br><br>\n",
    "1. This property is true in general. Use this to write a function called `detInv(A)` that computes the inverse of $A$ using determinants. Try to avoid computing cofactors twice!<br><br>\n",
    "1. Use your function to compute the inverse of the matrix from the first part of this question above.<br><br>\n",
    "1. Is this an efficient way to compute inverses? Experimentally show that once again, using row reduction is far better. (See [here](https://www.mathsisfun.com/algebra/matrix-inverse-row-operations-gauss-jordan.html) if you need a reminder as to how to find inverses using row reduction. Feel free to use the `rref(A,tol)` function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[-2,11,48],[-2,11,49],[3,-16,-70]])\n",
    "C = np.array([[14,7,-1],[2,-4,1],[11,2,0]])\n",
    "C= C.T\n",
    "d = det(A)\n",
    "print(A@C)\n",
    "print(d*np.eye(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detInv(A): \n",
    "    rows,cols = A.shape\n",
    "    i = 0\n",
    "    inv = np.zeros ((rows,cols))\n",
    "    for i in range (0,rows):\n",
    "        for j in range (0,cols):\n",
    "            B = np.delete(A,i, axis=0)\n",
    "            B = np.delete(B,j, axis=1)\n",
    "            inv[i,j] = (-1)**(i+j)* det(B)\n",
    "    return inv.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.  2. 11.]\n",
      " [ 7. -4.  2.]\n",
      " [-1.  1.  0.]]\n",
      "[[ 1.  0.  0. 14.  2. 11.]\n",
      " [ 0.  1.  0.  7. -4.  2.]\n",
      " [-0. -0.  1. -1.  1. -0.]]\n"
     ]
    }
   ],
   "source": [
    "print(detInv(A))\n",
    "rows,cols = np.shape(A)\n",
    "AA = np.zeros((rows,cols+cols))\n",
    "AA[0,cols]=1\n",
    "AA[1,cols+1]=1\n",
    "AA[2,cols+2]=1\n",
    "AA[0:rows,0:cols] = A\n",
    "print(rref(AA,10**-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387 µs ± 6.12 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "156 µs ± 2.07 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit detInv(A)\n",
    "%timeit rref(AA,10**-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is far more efficient to do row reduction in order to find the inverse of A, around 2x faster than using cofactors to find the inverse of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework Question 3\n",
    "\n",
    "It is often said that determinants are one of the least useful ways to do numerical computation with matrices. Do some research online and read a paper or two, then write a paragraph summarizing why this is the case. Include references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinants are often disreputed due to their extremely computationally inefficient nature. Methods to compute the determinant or to use the determinant to find another value/matrix (like using Cramer's rule to find the inverse of a matrix) are extremely inefficient, far more so than using simpler methods to compute the desired value. For instance, Axler denigrates the use of determinants to find eigenvalues, which we have yet to study. However, he demonstrates that it is completely possible to calculate these values sans the use of determinants, which typically take a very long time to compute.He states that there is virtually no linear algebra problem that can only be solved with the determinant value; even Cramer's rule, which heavily implements the calculation of determinants,\"is completely impractical\" (Axler).\n",
    "\n",
    "Axler, S. (1995). Down with determinants!. The American mathematical monthly, 102(2), 139-154."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
